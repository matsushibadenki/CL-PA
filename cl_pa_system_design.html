<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>自己進化型対話システム "CL-PA" 設計書</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
            line-height: 1.8;
            color: #333;
            background-color: #f9f9f9;
            margin: 0;
            padding: 20px;
        }
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: #ffffff;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.05);
        }
        h1, h2, h3 {
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
            margin-top: 40px;
        }
        h1 {
            text-align: center;
            border-bottom: none;
            font-size: 2.5em;
        }
        p, li {
            font-size: 1.1em;
        }
        code {
            background-color: #ecf0f1;
            padding: 2px 6px;
            border-radius: 4px;
            font-family: "Courier New", Courier, monospace;
        }
        pre {
            background-color: #2d3436;
            color: #dfe6e9;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 20px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        th, td {
            padding: 12px 15px;
            border: 1px solid #ddd;
            text-align: left;
        }
        th {
            background-color: #3498db;
            color: #ffffff;
        }
        tr:nth-child(even) {
            background-color: #f2f2f2;
        }
        .mermaid {
            text-align: center;
            background-color: #fff;
            padding: 20px;
            border-radius: 8px;
            border: 1px solid #eee;
            margin-top: 20px;
        }
        nav {
            background-color: #ecf0f1;
            padding: 15px;
            border-radius: 5px;
            margin-bottom: 30px;
        }
        nav h2 {
            border-bottom: none;
            margin-top: 0;
            font-size: 1.5em;
        }
        nav ul {
            list-style-type: none;
            padding: 0;
        }
        nav ul li a {
            text-decoration: none;
            color: #2980b9;
            font-size: 1.1em;
        }
        nav ul li a:hover {
            text-decoration: underline;
        }
        .note {
            background-color: #eaf2f8;
            border-left: 5px solid #3498db;
            padding: 15px;
            margin-top: 20px;
            border-radius: 0 5px 5px 0;
        }
        .warning {
            background-color: #fdf2e9;
            border-left: 5px solid #e67e22;
            padding: 15px;
            margin-top: 20px;
            border-radius: 0 5px 5px 0;
        }
        .success {
            background-color: #d5f4e6;
            border-left: 5px solid #27ae60;
            padding: 15px;
            margin-top: 20px;
            border-radius: 0 5px 5px 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>自己進化型対話システム "CL-PA" 設計書</h1>
        <p style="text-align: center; font-style: italic;">Continual Learning & Personalized Adaptation System</p>
        <p style="text-align: right;">更新日: 2025年9月8日</p>

        <nav>
            <h2>目次</h2>
            <ul>
                <li><a href="#section1">1. はじめに: プロジェクトの背景と目的</a></li>
                <li><a href="#section2">2. システムコンセプト: 破綻を防ぐ「3段階の記憶モデル」</a></li>
                <li><a href="#section3">3. システムアーキテクチャ: 応答と学習の分離</a></li>
                <li><a href="#section4">4. 実装アーキテクチャ: 性能と柔軟性を両立するハイブリッド構成</a></li>
                <li><a href="#section5">5. 主要コンポーネントと技術選定</a></li>
                <li><a href="#section6">6. 推論エンジンの選択に関する考察</a></li>
                <li><a href="#section7">7. セキュリティと品質管理</a></li>
                <li><a href="#section8">8. 運用とモニタリング</a></li>
                <li><a href="#section9">9. デプロイメント戦略</a></li>
                <li><a href="#section10">10. パフォーマンス指標と評価方法</a></li>
                <li><a href="#section11">11. まとめ</a></li>
            </ul>
        </nav>

        <section id="section1">
            <h2>1. はじめに: プロジェクトの背景と目的</h2>
            <p>
                現在の大規模言語モデル（LLM）は、非常に高い対話能力を持っていますが、その知識は学習データが作られた時点に固定されています。そのため、一度リリースされると、ユーザーとの対話を通じて新しい情報を学んだり、個々のユーザーに合わせて応答を最適化したりすることは基本的にありません。これは、まるで「一度も成長しない博識なアシスタント」を相手にしているようなものです。
            </p>
            <p>
                本プロジェクトの目的は、この静的なLLMの制約を打破することです。ユーザーとの対話をリアルタイムで学習し、その内容を応答に反映させ、さらには安定した知識として自身に統合していく<strong>「自己進化型」</strong>の対話システムを構築します。これにより、以下のような体験の実現を目指します。
            </p>
            <ul>
                <li><strong>パーソナライゼーション:</strong> ユーザーの名前、好み、過去の対話内容を記憶し、よりパーソナルな応答を返す。</li>
                <li><strong>継続的学習:</strong> 新しい情報やユーザーからの指摘を学習し、常に知識をアップデートし続ける。</li>
                <li><strong>安定性の確保:</strong> リアルタイム性を追求しつつも、誤った情報を学習してモデル全体が破綻することを防ぐ堅牢な仕組みを持つ。</li>
            </ul>
            <p class="note">
                この設計書は、上記目標を達成するためのシステム「CL-PA (Continual Learning & Personalized Adaptation)」の全体像、アーキテクチャ、および技術選定について、経緯を知らない方でも理解できるよう網羅的に解説するものです。
            </p>
        </section>

        <section id="section2">
            <h2>2. システムコンセプト: 破綻を防ぐ「3段階の記憶モデル」</h2>
            <p>
                リアルタイム学習の最大の課題は、新しい情報を学ぶたびに過去の重要な知識を忘れてしまう「破滅的忘却」や、誤った情報を鵜呑みにしてしまうリスクです。これを防ぐため、本システムは人間の記憶メカニズムを参考に<strong>「短期記憶」「中期記憶」「長期記憶」</strong>の3段階で学習内容をフィルタリングし、定着させていくアプローチを採用します。
            </p>
            <h3>Phase 1: 短期記憶 (Volatile LoRA)</h3>
            <p>
                <strong>役割:</strong> 最新の対話内容を即座に反映させ、「さっき言ったことを覚えている」という体験を提供します。<br>
                <strong>仕組み:</strong> 対話の数ターンごとに、ごく一部のデータで小さな追加学習モジュール（LoRA）を高速に更新します。この記憶は揮発性で、安定性よりも即時性を重視します。<br>
                <strong>更新頻度:</strong> 1-5ターンごと（約1-3分間隔）
            </p>
            <h3>Phase 2: 中期記憶 (Stable LoRA)</h3>
            <p>
                <strong>役割:</strong> 一連の対話（セッション）を通じて得られたユーザーの一貫した特性や重要事項を、より安定した形で記憶します。<br>
                <strong>仕組み:</strong> セッション終了時に、その対話全体と過去の重要事項データを合わせて、より大きなLoRAを学習します。これにより、短期記憶のノイズが除去され、安定したパーソナライゼーションが実現します。<br>
                <strong>更新頻度:</strong> セッション終了時（数時間から1日間隔）
            </p>
            <h3>Phase 3: 長期記憶 (Base Model Merge)</h3>
            <p>
                <strong>役割:</strong> 全ユーザーに共通して有益な知識や、極めて高品質な学習成果を、LLM本体の知識として恒久的に統合します。<br>
                <strong>仕組み:</strong> 週に一度などの定期的なタイミングで、十分に検証された「中期記憶」の中から特に優れたものを厳格な品質テストにかけます。テストに合格したもののみが、LLM本体にマージ（統合）され、モデルの基本性能が恒久的に向上します。<br>
                <strong>更新頻度:</strong> 週次または月次
            </p>
            <div class="note">
                <strong>LoRAとは？</strong> LoRA (Low-Rank Adaptation) は、LLMの巨大な本体（数十億パラメータ）を凍結したまま、小さな追加層（数百万パラメータ）だけを学習させる技術です。これにより、計算コストを劇的に抑えながら、モデルを特定のタスクや知識に合わせて効率的にファインチューニングできます。本システムではこのLoRAを「記憶モジュール」として活用します。
            </div>
        </section>

        <section id="section3">
            <h2>3. システムアーキテクチャ: 応答と学習の分離</h2>
            <p>
                システムの安定性とリアルタイム性を両立させるため、ユーザーに応答を返す「オンライン・プロセス」と、モデルを学習させる「オフライン・プロセス」を完全に分離します。これにより、重い学習処理がユーザーの応答遅延を引き起こすことはありません。
            </p>
            <div class="mermaid">
                graph TB
                    subgraph "オンライン・プロセス（リアルタイム応答）"
                        User["ユーザー"]
                        User -- "プロンプト入力" --> InferenceEngine["対話エンジン"]
                        InferenceEngine -- "LoRAを読み込み" --> LoRARepository
                        LoRARepository -- "Stable + Volatile LoRA提供" --> InferenceEngine
                        InferenceEngine -- "応答生成" --> User
                        InferenceEngine -- "対話ログ保存" --> LogDB["対話ログDB"]
                    end

                    subgraph "オフライン・プロセス（非同期学習）"
                        LogDB -- "ログをトリガー" --> LearningManager["学習マネージャー"]
                        LearningManager -- "短期学習指示" --> Phase1["Phase 1: 短期記憶学習"]
                        Phase1 -- "Volatile LoRA更新" --> LoRARepository
                        
                        LearningManager -- "中期学習指示" --> Phase2["Phase 2: 中期記憶学習"]
                        Phase2 -- "Stable LoRA更新" --> LoRARepository
                        
                        LoRARepository[("LoRAリポジトリ")]

                        subgraph "週次バッチ・プロセス"
                            LoRARepository -- "Candidate LoRA提出" --> QAPipeline
                            QAPipeline["QA & マージパイプライン"]
                            BaseLLM_Old["ベースLLM"] -- "マージ実行" --> QAPipeline
                            QAPipeline -- "品質評価テスト" --> MergeDecision{"Merge?"}
                            MergeDecision -- "Yes" --> BaseLLM_New["ベースLLM（更新版）"]
                            MergeDecision -- "No" --> Reject["棄却"]
                        end
                    end
            </div>
        </section>

        <section id="section4">
            <h2>4. 実装アーキテクチャ: 性能と柔軟性を両立するハイブリッド構成</h2>
            <p>
                本システムの実装にあたり、各コンポーネントの特性を最大限に活かすため、異なる技術を組み合わせた<strong>ハイブリッド構成</strong>を採用します。具体的には、高速な推論が得意な<code>Llama.cpp</code>と、機械学習のエコシステムが豊富な<code>Python</code>を組み合わせます。
            </p>
            <ul>
                <li><strong>推論エンジン (C++):</strong> <code>Llama.cpp</code>をサーバーとして利用します。CPUや一般のGPUでも高速に応答を生成し、動的にLoRAを適用する役割を担います。</li>
                <li><strong>システム全体制御と学習 (Python):</strong> APIサーバー、学習管理、実際のLoRA学習、品質評価とマージといった、推論以外のすべてのロジックをPythonのエコシステムで構築します。</li>
            </ul>
            <p>
                この構成により、<code>Llama.cpp</code>の最高の推論性能と、Pythonの最高の開発柔軟性という「良いとこ取り」を実現します。
            </p>
            <div class="mermaid">
                graph TB
                    subgraph "ユーザーインターフェース"
                        WebAppFE["Web/App フロントエンド"]
                    end

                    subgraph "システムバックエンド（Python）"
                        WebAppFE -- "API Request" --> API["APIサーバー"]
                        API -- "推論リクエスト" --> LlamaCppServer
                        API -- "対話ログ保存" --> LogDB
                        API -- "学習タスク投入" --> TaskQueue
                    end

                    subgraph "推論エンジン（C++）"
                        LlamaCppServer["Llama.cpp Server"]
                        LlamaCppServer -- "LoRAロード" --> LoRARepo
                        LlamaCppServer -- "モデルロード" --> BaseModel
                    end

                    subgraph "非同期学習・管理（Python）"
                        TaskQueue["タスクキュー"]
                        TaskQueue -- "タスク処理" --> LearningWorker
                        LearningWorker["学習ワーカー"]
                        LearningWorker -- "ログ読み込み" --> LogDB
                        LearningWorker -- "LoRA学習" --> MLFrameworks
                        LearningWorker -- "学習済みLoRA保存" --> LoRARepo
                        MLFrameworks["MLライブラリ"]
                        
                        Cron["スケジューラ"] -- "週次実行" --> QAPipeline
                        QAPipeline["QA & マージパイプライン"]
                        QAPipeline -- "評価・マージ" --> LoRARepo
                        QAPipeline -- "ベースモデル更新" --> BaseModel
                    end

                    subgraph "データストア"
                        BaseModel["ベースモデル"]
                        LoRARepo["LoRAリポジトリ"]
                        LogDB["対話ログDB"]
                    end
            </div>
        </section>

        <section id="section5">
            <h2>5. 主要コンポーネントと技術選定</h2>
            <p>
                ハイブリッド構成を実現するための具体的なソフトウェアおよびライブラリの選定は以下の通りです。
            </p>
            <table>
                <thead>
                    <tr>
                        <th>役割</th>
                        <th>コンポーネント</th>
                        <th>推奨ソフトウェア/ライブラリ</th>
                        <th>選定理由</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>推論エンジン</strong></td>
                        <td><code>Llama.cpp</code> Server</td>
                        <td><strong><code>Llama.cpp</code></strong></td>
                        <td>GGUFモデルの高速推論、マルチLoRA対応、CPU/GPUでの優れたパフォーマンス。C++製で軽量。</td>
                    </tr>
                    <tr>
                        <td><strong>APIサーバー</strong></td>
                        <td>WebApp Backend</td>
                        <td><strong><code>FastAPI</code></strong> (Python)</td>
                        <td>非同期処理に強く、高速。Pythonエコシステムとの親和性が非常に高い。</td>
                    </tr>
                    <tr>
                        <td><strong>非同期処理</strong></td>
                        <td>タスクキュー</td>
                        <td><strong><code>Celery</code></strong> with <strong><code>RabbitMQ</code></strong> or <strong><code>Redis</code></strong></td>
                        <td>時間のかかるLoRA学習を非同期で実行するため。システムの応答性を担保する不可欠な要素。</td>
                    </tr>
                    <tr>
                        <td rowspan="4"><strong>機械学習</strong></td>
                        <td rowspan="4">学習ワーカー / QAパイプライン</td>
                        <td><strong><code>PyTorch</code></strong></td>
                        <td>最新の機械学習研究を支えるデファクトスタンダード。</td>
                    </tr>
                    <tr>
                        <td><strong><code>Hugging Face Transformers</code></strong></td>
                        <td>モデルやトークナイザーの読み込み、データセットの扱いに必須。</td>
                    </tr>
                    <tr>
                        <td><strong><code>Hugging Face PEFT</code></strong></td>
                        <td>LoRAの学習、マージ、管理を簡単に行うためのライブラリ。本システムの核心部分。</td>
                    </tr>
                    <tr>
                        <td><strong><code>Hugging Face Accelerate</code></strong></td>
                        <td>複数GPUや効率的な学習をサポート。</td>
                    </tr>
                    <tr>
                        <td><strong>データベース</strong></td>
                        <td>対話ログDB</td>
                        <td><strong><code>MongoDB</code></strong> or <strong><code>PostgreSQL</code></strong></td>
                        <td>JSON形式の対話ログと相性が良い<code>MongoDB</code>か、信頼性が高い<code>PostgreSQL</code>を要件に応じて選択。</td>
                    </tr>
                    <tr>
                        <td><strong>ストレージ</strong></td>
                        <td>LoRA / モデルリポジトリ</td>
                        <td>ローカルファイルシステム or <strong><code>Amazon S3</code></strong> / <strong><code>GCS</code></strong></td>
                        <td>学習済みのLoRAファイルやGGUFモデルを保存。スケールを考えるならクラウドストレージが望ましい。</td>
                    </tr>
                    <tr>
                        <td><strong>バッチ処理</strong></td>
                        <td>スケジューラ</td>
                        <td><strong><code>Cron</code></strong> (OS標準) or <strong><code>Apache Airflow</code></strong></td>
                        <td>週次のQA・マージパイプラインを定期実行するため。小規模なら<code>Cron</code>で十分。</td>
                    </tr>
                </tbody>
            </table>
        </section>
        
        <section id="section6">
            <h2>6. 推論エンジンの選択に関する考察</h2>
            <p>
                本設計では<code>Llama.cpp</code>を初期の推論エンジンとして推奨しますが、システムの将来的なスケールによっては他の選択肢も視野に入ります。推論エンジンの選択は、システムのターゲット環境と想定ユーザー数に大きく依存します。
            </p>
            <table>
                <thead>
                    <tr>
                        <th>評価軸</th>
                        <th>Llama.cpp (本設計の推奨)</th>
                        <th>vLLM (大規模サービス向け)</th>
                        <th>TensorRT-LLM (最高性能追求)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>主な実行環境</strong></td>
                        <td><strong>CPU, PC, Mac, コンシューマーGPU</strong></td>
                        <td><strong>NVIDIA GPUサーバー</strong></td>
                        <td><strong>NVIDIA GPUサーバー</strong></td>
                    </tr>
                    <tr>
                        <td><strong>スループット（同時処理性能）</strong></td>
                        <td>△ (個人・小規模向け)</td>
                        <td><strong>◎ (大規模向け)</strong></td>
                        <td>○ (大規模向け)</td>
                    </tr>
                    <tr>
                        <td><strong>レイテンシ（応答速度）</strong></td>
                        <td>○</td>
                        <td>○</td>
                        <td><strong>◎ (特定環境で最速)</strong></td>
                    </tr>
                    <tr>
                        <td><strong>リソース効率</strong></td>
                        <td><strong>◎ (特にCPU/メモリ)</strong></td>
                        <td>○ (GPUメモリ効率は高い)</td>
                        <td>○</td>
                    </tr>
                    <tr>
                        <td><strong>導入・開発の容易さ</strong></td>
                        <td><strong>◎</strong></td>
                        <td>○</td>
                        <td>△</td>
                    </tr>
                </tbody>
            </table>
            <h3>推奨アプローチ</h3>
            <ol>
                <li><strong>フェーズ1 (プロトタイプ・初期開発):</strong> まずは<strong><code>Llama.cpp</code></strong>でシステム全体を構築します。環境を選ばない柔軟性により、迅速な開発とテストが可能です。</li>
                <li><strong>フェーズ2 (サービス拡大期):</strong> システムのユーザー数が増加し、より高いスループットが求められるようになった時点で、推論エンジン部分を<strong><code>vLLM</code></strong>に載せ替えることを検討します。アーキテクチャが疎結合であるため、この移行は比較的容易に行えます。</li>
            </ol>
        </section>

        <section id="section7">
            <h2>7. セキュリティと品質管理</h2>
            <h3>品質管理パイプライン</h3>
            <p>
                長期記憶への統合前に、学習内容の品質を厳格に評価するシステムが必要です。
            </p>
            <ul>
                <li><strong>事実性検証:</strong> 学習された情報が既知の事実と矛盾していないかを自動検証</li>
                <li><strong>安全性チェック:</strong> 有害・偏見・差別的な内容が含まれていないかをフィルタリング</li>
                <li><strong>回帰テスト:</strong> 新しい学習がモデルの既存機能を損なっていないかを確認</li>
                <li><strong>A/Bテスト:</strong> 統合前後での応答品質を比較評価</li>
            </ul>

            <h3>セキュリティ対策</h3>
            <ul>
                <li><strong>プロンプトインジェクション対策:</strong> 悪意のある学習指示を防ぐフィルタリング</li>
                <li><strong>データプライバシー:</strong> 個人情報の自動検出と匿名化処理</li>
                <li><strong>アクセス制御:</strong> 学習データとモデルファイルへの適切な権限管理</li>
                <li><strong>監査ログ:</strong> すべての学習と統合の履歴を追跡可能な形で記録</li>
            </ul>

            <div class="warning">
                <strong>重要:</strong> 品質管理とセキュリティ対策は、システムの信頼性を確保する上で絶対に妥協できない要素です。特に長期記憶への統合時には、複数段階の検証プロセスを必須とします。
            </div>
        </section>

        <section id="section8">
            <h2>8. 運用とモニタリング</h2>
            <h3>システム監視項目</h3>
            <table>
                <thead>
                    <tr>
                        <th>監視カテゴリ</th>
                        <th>主要指標</th>
                        <th>アラート閾値例</th>
                        <th>対応アクション</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>応答性能</strong></td>
                        <td>応答時間、スループット、同時接続数</td>
                        <td>応答時間 > 5秒、エラー率 > 5%</td>
                        <td>推論エンジンのスケールアップ、LoRA最適化</td>
                    </tr>
                    <tr>
                        <td><strong>学習品質</strong></td>
                        <td>学習成功率、品質スコア、統合承認率</td>
                        <td>学習失敗率 > 10%、品質スコア < 0.7</td>
                        <td>学習データの再検証、パラメータ調整</td>
                    </tr>
                    <tr>
                        <td><strong>リソース使用量</strong></td>
                        <td>CPU、GPU、メモリ使用率</td>
                        <td>使用率 > 85%、メモリ不足警告</td>
                        <td>リソース追加、処理の最適化</td>
                    </tr>
                    <tr>
                        <td><strong>データ整合性</strong></td>
                        <td>LoRA整合性、DB同期状態</td>
                        <td>データ破損検出、同期エラー</td>
                        <td>バックアップからの復旧、データ修復</td>
                    </tr>
                </tbody>
            </table>

            <h3>ログ管理戦略</h3>
            <ul>
                <li><strong>構造化ログ:</strong> JSON形式での統一されたログフォーマット</li>
                <li><strong>分散トレーシング:</strong> リクエストの全ライフサイクルを追跡</li>
                <li><strong>メトリクス収集:</strong> Prometheus + Grafanaによる可視化</li>
                <li><strong>アラート管理:</strong> Slack/Discord統合による即時通知</li>
            </ul>

            <div class="mermaid">
                graph LR
                    subgraph "監視・アラート基盤"
                        Logs["各種ログ"] --> LogAggregator["ログ集約器"]
                        Metrics["メトリクス"] --> Prometheus["Prometheus"]
                        Prometheus --> Grafana["Grafana Dashboard"]
                        Prometheus --> AlertManager["Alert Manager"]
                        AlertManager --> Slack["Slack通知"]
                        AlertManager --> PagerDuty["PagerDuty"]
                    end
            </div>
        </section>

        <section id="section9">
            <h2>9. デプロイメント戦略</h2>
            <h3>段階的デプロイメント</h3>
            <p>
                システムの複雑性を考慮し、以下の段階的なデプロイメント戦略を推奨します。
            </p>

            <div class="success">
                <strong>Stage 1: 基本応答システム</strong><br>
                - Llama.cppによる基本的な対話機能<br>
                - 静的な応答のみ（学習機能なし）<br>
                - 期間: 2-4週間
            </div>

            <div class="success">
                <strong>Stage 2: 短期記憶の実装</strong><br>
                - Volatile LoRAによる短期記憶機能<br>
                - セッション内での学習・記憶<br>
                - 期間: 4-6週間
            </div>

            <div class="success">
                <strong>Stage 3: 中期記憶の実装</strong><br>
                - Stable LoRAによる中期記憶機能<br>
                - セッション間での記憶継続<br>
                - 期間: 6-8週間
            </div>

            <div class="success">
                <strong>Stage 4: 長期記憶と品質管理</strong><br>
                - Base Model Mergeによる長期記憶<br>
                - 品質評価パイプラインの実装<br>
                - 期間: 8-12週間
            </div>

            <h3>コンテナ化とオーケストレーション</h3>
            <pre><code># docker-compose.yml の例
version: '3.8'
services:
  api-server:
    build: ./api-server
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=mongodb://mongo:27017
    depends_on:
      - mongo
      - redis
      - llama-cpp-server
  
  llama-cpp-server:
    image: llamacpp/server
    ports:
      - "8080:8080"
    volumes:
      - ./models:/models
      - ./loras:/loras
    command: --model /models/base-model.gguf --port 8080
  
  learning-worker:
    build: ./learning-worker
    depends_on:
      - redis
      - mongo
    environment:
      - CELERY_BROKER_URL=redis://redis:6379
  
  mongo:
    image: mongo:7
    volumes:
      - mongodb_data:/data/db
  
  redis:
    image: redis:7-alpine</code></pre>

            <h3>CI/CDパイプライン</h3>
            <ul>
                <li><strong>自動テスト:</strong> ユニットテスト、統合テスト、品質回帰テスト</li>
                <li><strong>段階的デプロイ:</strong> Dev → Staging → Production</li>
                <li><strong>ロールバック機能:</strong> 問題発生時の迅速な巻き戻し</li>
                <li><strong>モデルバージョン管理:</strong> Git LFSまたはDVCによるモデルファイル管理</li>
            </ul>
        </section>

        <section id="section10">
            <h2>10. パフォーマンス指標と評価方法</h2>
            <h3>システム性能指標</h3>
            <table>
                <thead>
                    <tr>
                        <th>指標カテゴリ</th>
                        <th>具体的指標</th>
                        <th>目標値</th>
                        <th>測定方法</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td rowspan="3"><strong>応答性能</strong></td>
                        <td>平均応答時間</td>
                        <td>< 3秒</td>
                        <td>APIレスポンス時間の継続監視</td>
                    </tr>
                    <tr>
                        <td>95パーセンタイル応答時間</td>
                        <td>< 10秒</td>
                        <td>分散トレーシングによる詳細分析</td>
                    </tr>
                    <tr>
                        <td>同時処理可能ユーザー数</td>
                        <td>> 100人（初期）</td>
                        <td>負荷テストによる実測</td>
                    </tr>
                    <tr>
                        <td rowspan="3"><strong>学習効果</strong></td>
                        <td>記憶精度</td>
                        <td>> 85%</td>
                        <td>標準化テストセットでの評価</td>
                    </tr>
                    <tr>
                        <td>パーソナライゼーション効果</td>
                        <td>満足度 > 4.0/5.0</td>
                        <td>ユーザーアンケートによる主観評価</td>
                    </tr>
                    <tr>
                        <td>知識統合成功率</td>
                        <td>> 70%</td>
                        <td>品質評価パイプラインの統計</td>
                    </tr>
                    <tr>
                        <td rowspan="2"><strong>システム安定性</strong></td>
                        <td>稼働率</td>
                        <td>> 99.5%</td>
                        <td>アップタイム監視</td>
                    </tr>
                    <tr>
                        <td>データ損失率</td>
                        <td>< 0.01%</td>
                        <td>バックアップ・復旧テスト</td>
                    </tr>
                </tbody>
            </table>

            <h3>評価用データセット</h3>
            <ul>
                <li><strong>記憶テスト:</strong> 名前、好み、過去の発言の記憶精度を測定</li>
                <li><strong>学習テスト:</strong> 新情報の獲得と活用能力を評価</li>
                <li><strong>安全性テスト:</strong> 有害な内容の学習・出力を防げているかを確認</li>
                <li><strong>一貫性テスト:</strong> 学習による既存知識への悪影響がないかを検証</li>
            </ul>

            <div class="mermaid">
                graph TD
                    subgraph "評価フレームワーク"
                        TestData["標準テストデータセット"]
                        TestData --> MemoryTest["記憶精度テスト"]
                        TestData --> LearningTest["学習効果テスト"]
                        TestData --> SafetyTest["安全性テスト"]
                        TestData --> ConsistencyTest["一貫性テスト"]
                        
                        MemoryTest --> Scoring["総合スコア算出"]
                        LearningTest --> Scoring
                        SafetyTest --> Scoring
                        ConsistencyTest --> Scoring
                        
                        Scoring --> Report["評価レポート"]
                        Report --> Dashboard["監視ダッシュボード"]
                    end
            </div>
        </section>

        <section id="section11">
            <h2>11. まとめ</h2>
            <p>
                本設計書で提案した「CL-PA System」は、3段階の記憶モデルと、性能と柔軟性を両立するハイブリッドアーキテクチャを採用することで、LLMがユーザーとの対話を通じて自己進化していく未来を実現します。
            </p>
            
            <h3>主要な設計上の利点</h3>
            <ul>
                <li><strong>段階的な品質管理:</strong> 3段階のメモリシステムにより、即座の応答性と長期的な安定性を両立</li>
                <li><strong>技術的柔軟性:</strong> ハイブリッドアーキテクチャにより、各技術の長所を最大限活用</li>
                <li><strong>運用の現実性:</strong> 段階的デプロイメントと詳細な監視により、実際の運用において高い信頼性を確保</li>
                <li><strong>スケーラビリティ:</strong> 初期は小規模で開始し、需要に応じて大規模システムへの移行が可能</li>
            </ul>

            <h3>期待される効果</h3>
            <p>
                このシステムは、静的だったAIを、一人ひとりに寄り添い、共に成長していく動的なパートナーへと変える可能性を秘めています。初期実装は<code>Llama.cpp</code>をベースに進め、将来的なスケールにも対応可能な、発展性のある設計となっています。
            </p>

            <div class="note">
                <strong>今後の展望:</strong> 本システムの成功により、AIとの対話は単なる情報のやり取りから、継続的な学習・成長を共有する新しい形のコミュニケーションへと進化することが期待されます。これは、AI技術の新たな可能性を切り開く革新的なアプローチとなるでしょう。
            </div>

            <h3>実装開始のための次のステップ</h3>
            <ol>
                <li><strong>技術検証:</strong> Llama.cpp + Python環境での基本的な統合テスト</li>
                <li><strong>データ設計:</strong> 対話ログとLoRA管理のためのデータベーススキーマ設計</li>
                <li><strong>プロトタイプ開発:</strong> Stage 1（基本応答システム）の実装開始</li>
                <li><strong>評価基盤構築:</strong> システム性能と学習効果を測定するためのテスト環境整備</li>
            </ol>
        </section>

    </div>

    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ 
            startOnLoad: true,
            theme: 'default',
            themeVariables: {
                primaryColor: '#3498db',
                primaryTextColor: '#2c3e50',
                primaryBorderColor: '#2980b9',
                lineColor: '#34495e',
                secondaryColor: '#ecf0f1',
                tertiaryColor: '#ffffff'
            }
        });
    </script>
</body>
</html>